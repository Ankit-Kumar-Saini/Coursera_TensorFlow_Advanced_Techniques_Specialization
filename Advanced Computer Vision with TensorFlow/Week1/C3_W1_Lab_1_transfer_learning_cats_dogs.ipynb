{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C3_W1_Lab_1_transfer_learning_cats_dogs.ipynb","provenance":[{"file_id":"1JGWdKozbc3dbRzFUtqqCLXbnsFg7EPtv","timestamp":1608904631886}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fYJqjq66JVQQ"},"source":["# Basic transfer learning with cats and dogs data\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0oWuHhhcJVQQ"},"source":["### Import tensorflow"]},{"cell_type":"code","metadata":{"id":"ioLbtB3uGKPX","executionInfo":{"status":"ok","timestamp":1614527134217,"user_tz":-330,"elapsed":1812,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}}},"source":["try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjfMJAHPJVQR"},"source":["### Import modules and download the cats and dogs dataset."]},{"cell_type":"code","metadata":{"id":"y23ucAFLoHop","executionInfo":{"status":"ok","timestamp":1614516251702,"user_tz":-330,"elapsed":19126,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}}},"source":["import urllib.request\n","import os\n","import zipfile\n","import random\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.optimizers import RMSprop\n","from shutil import copyfile\n","\n","\n","data_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\"\n","data_file_name = \"catsdogs.zip\"\n","download_dir = '/tmp/'\n","urllib.request.urlretrieve(data_url, data_file_name)\n","zip_ref = zipfile.ZipFile(data_file_name, 'r')\n","zip_ref.extractall(download_dir)\n","zip_ref.close()"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNVXCUNUJVQR"},"source":["Check that the dataset has the expected number of examples."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwMoZHxWOynx","executionInfo":{"status":"ok","timestamp":1614516331055,"user_tz":-330,"elapsed":1044,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"0c7549ff-4f20-4c04-f7c4-17f9adfe0c76"},"source":["print(\"Number of cat images:\",len(os.listdir('/tmp/PetImages/Cat/')))\n","print(\"Number of dog images:\", len(os.listdir('/tmp/PetImages/Dog/')))\n","\n","# Expected Output:\n","# Number of cat images: 12501\n","# Number of dog images: 12501"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Number of cat images: 12501\n","Number of dog images: 12501\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_0riaptkJVQR"},"source":["Create some folders that will store the training and test data.\n","- There will be a training folder and a testing folder.\n","- Each of these will have a subfolder for cats and another subfolder for dogs."]},{"cell_type":"code","metadata":{"id":"qygIo4W5O1hQ","executionInfo":{"status":"ok","timestamp":1614516419629,"user_tz":-330,"elapsed":985,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}}},"source":["try:\n","    os.mkdir('/tmp/cats-v-dogs')\n","    os.mkdir('/tmp/cats-v-dogs/training')\n","    os.mkdir('/tmp/cats-v-dogs/testing')\n","    os.mkdir('/tmp/cats-v-dogs/training/cats')\n","    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n","    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n","    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n","except OSError:\n","    pass"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ZHD_c-sJVQR"},"source":["### Split data into training and test sets\n","\n","- The following code put first checks if an image file is empty (zero length)\n","- Of the files that are not empty, it puts 90% of the data into the training set, and 10% into the test set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M90EiIu0O314","executionInfo":{"status":"ok","timestamp":1614516666097,"user_tz":-330,"elapsed":4519,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"072a2a2b-1314-446c-870d-b5191b9d54db"},"source":["import random\n","def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n","    files = []\n","    for filename in os.listdir(SOURCE):\n","        file = SOURCE + filename\n","        if os.path.getsize(file) > 0:\n","            files.append(filename)\n","        else:\n","            print(filename + \" is zero length, so ignoring.\")\n","\n","    training_length = int(len(files) * SPLIT_SIZE)\n","    testing_length = int(len(files) - training_length)\n","    shuffled_set = random.sample(files, len(files))\n","    training_set = shuffled_set[0:training_length]\n","    testing_set = shuffled_set[training_length:]\n","\n","    for filename in training_set:\n","        this_file = SOURCE + filename\n","        destination = TRAINING + filename\n","        copyfile(this_file, destination)\n","\n","    for filename in testing_set:\n","        this_file = SOURCE + filename\n","        destination = TESTING + filename\n","        copyfile(this_file, destination)\n","\n","\n","CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n","TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n","TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n","DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n","TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n","TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n","\n","split_size = .9\n","split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n","split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n","\n","# Expected output\n","# 666.jpg is zero length, so ignoring\n","# 11702.jpg is zero length, so ignoring"],"execution_count":5,"outputs":[{"output_type":"stream","text":["666.jpg is zero length, so ignoring.\n","11702.jpg is zero length, so ignoring.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KMx_pePuJVQR"},"source":["Check that the training and test sets are the expected lengths."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cl8sQpM1O9xK","executionInfo":{"status":"ok","timestamp":1614516679512,"user_tz":-330,"elapsed":960,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"aed1f077-c0e8-4311-d211-6c2836495683"},"source":["\n","print(\"Number of training cat images\", len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n","print(\"Number of training dog images\", len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n","print(\"Number of testing cat images\", len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n","print(\"Number of testing dog images\", len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n","\n","# expected output\n","# Number of training cat images 11250\n","# Number of training dog images 11250\n","# Number of testing cat images 1250\n","# Number of testing dog images 1250"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Number of training cat images 11250\n","Number of training dog images 11250\n","Number of testing cat images 1250\n","Number of testing dog images 1250\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pNz89__rJVQR"},"source":["### Data augmentation (try adjusting the parameters)!\n","\n","Here, you'll use the `ImageDataGenerator` to perform data augmentation.  \n","- Things like rotating and flipping the existing images allows you to generate training data that is more varied, and can help the model generalize better during training.  \n","- You can also use the data generator to apply data augmentation to the validation set.\n","\n","You can use the default parameter values for a first pass through this lab.\n","- Later, try to experiment with the parameters of `ImageDataGenerator` to improve the model's performance.\n","- Try to drive reach 99.9% validation accuracy or better."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVO1l8vAPE14","executionInfo":{"status":"ok","timestamp":1614527147849,"user_tz":-330,"elapsed":2880,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"85910f59-be92-48bb-f55b-9210229898e2"},"source":["TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n","# Experiment with your own parameters to reach 99.9% validation accuracy or better\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","      rotation_range = 40,\n","      width_shift_range = 0.2,\n","      height_shift_range = 0.2,\n","      shear_range = 0.2,\n","      zoom_range = 0.2,\n","      brightness_range = (0.2, 2.0),\n","      horizontal_flip = True,\n","      fill_mode = 'nearest')\n","\n","train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n","                                                    batch_size = 128,\n","                                                    class_mode = 'binary',\n","                                                    target_size = (150, 150))\n","\n","VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n","\n","validation_datagen = ImageDataGenerator(rescale = 1./255)\n","validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n","                                                              batch_size = 128,\n","                                                              class_mode = 'binary',\n","                                                              target_size = (150, 150))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Found 22498 images belonging to 2 classes.\n","Found 2500 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WchwDzWNJVQR"},"source":["### Get and prepare the model\n","\n","You'll be using the `InceptionV3` model.  \n","- Since you're making use of transfer learning, you'll load the pre-trained weights of the model.\n","- You'll also freeze the existing layers so that they aren't trained on your downstream task with the cats and dogs data.\n","- You'll also get a reference to the last layer, 'mixed7' because you'll add some layers after this last layer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiPK1LlMOvm7","executionInfo":{"status":"ok","timestamp":1614527158699,"user_tz":-330,"elapsed":4201,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"b18218d5-fcbc-4340-d3b6-58f0bd11086b"},"source":["weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n","weights_file = \"inception_v3.h5\"\n","urllib.request.urlretrieve(weights_url, weights_file)\n","\n","# Instantiate the model\n","pre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n","                                include_top = False,\n","                                weights = None)\n","\n","# load pre-trained weights\n","pre_trained_model.load_weights(weights_file)\n","\n","# freeze the layers\n","for layer in pre_trained_model.layers:\n","    layer.trainable = False\n","\n","# pre_trained_model.summary()\n","\n","last_layer = pre_trained_model.get_layer('mixed7')\n","print('last layer output shape: ', last_layer.output_shape)\n","last_output = last_layer.output\n"],"execution_count":29,"outputs":[{"output_type":"stream","text":["last layer output shape:  (None, 7, 7, 768)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3edBz_IxJVQR"},"source":["### Add layers\n","Add some layers that you will train on the cats and dogs data.\n","- `Flatten`: This will take the output of the `last_layer` and flatten it to a vector.\n","- `Dense`: You'll add a dense layer with a relu activation.\n","- `Dense`: After that, add a dense layer with a sigmoid activation.  The sigmoid will scale the output to range from 0 to 1, and allow you to interpret the output as a prediction between two categories (cats or dogs).\n","\n","Then create the model object."]},{"cell_type":"code","metadata":{"id":"oDidHXO1JVQR","executionInfo":{"status":"ok","timestamp":1614527180609,"user_tz":-330,"elapsed":1680,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}}},"source":["# Flatten the output layer to 1 dimension\n","x = layers.GlobalAveragePooling2D()(last_output)\n","# Add a fully connected layer with 1,024 hidden units and ReLU activation\n","x = layers.Dense(512, activation = 'relu')(x)\n","x = layers.Dropout(0.3)(x)\n","x = layers.Dense(128, activation = 'relu')(x)\n","# Add a final sigmoid layer for classification\n","x = layers.Dense(1, activation = 'sigmoid')(x)\n","\n","model = Model(pre_trained_model.input, x)"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asCm8okXJVQR"},"source":["### Train the model\n","Compile the model, and then train it on the test data using `model.fit`\n","- Feel free to adjust the number of epochs.  This project was originally designed with 20 epochs.\n","- For the sake of time, you can use fewer epochs (2) to see how the code runs.\n","- You can ignore the warnings about some of the images having corrupt EXIF data. Those will be skipped."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3nxUncKWPRhR","executionInfo":{"status":"ok","timestamp":1614529670818,"user_tz":-330,"elapsed":584502,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"ad5fbfee-795f-4f30-fbd6-1388cce97af9"},"source":["# compile the model\n","model.compile(optimizer = RMSprop(lr = 0.0001),\n","              loss = 'binary_crossentropy',\n","              metrics = ['acc'])\n","\n","# train the model (adjust the number of epochs from 1 to improve performance)\n","history = model.fit(\n","            train_generator,\n","            validation_data = validation_generator,\n","            epochs = 3,\n","            verbose = 1)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n"," 48/176 [=======>......................] - ETA: 2:14 - loss: 0.1423 - acc: 0.9398"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n","  warnings.warn(str(msg))\n"],"name":"stderr"},{"output_type":"stream","text":["176/176 [==============================] - 197s 1s/step - loss: 0.1463 - acc: 0.9380 - val_loss: 0.0590 - val_acc: 0.9764\n","Epoch 2/3\n","176/176 [==============================] - 193s 1s/step - loss: 0.1428 - acc: 0.9406 - val_loss: 0.0583 - val_acc: 0.9752\n","Epoch 3/3\n","176/176 [==============================] - 193s 1s/step - loss: 0.1416 - acc: 0.9395 - val_loss: 0.0677 - val_acc: 0.9748\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1HaovESYQCVY","executionInfo":{"status":"ok","timestamp":1614530606466,"user_tz":-330,"elapsed":1155,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}}},"source":["for layer in pre_trained_model.layers[:197]:\r\n","   layer.trainable = False\r\n","for layer in pre_trained_model.layers[197:]:\r\n","   layer.trainable = True"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XREVYMGWQZPK","executionInfo":{"status":"ok","timestamp":1614532520734,"user_tz":-330,"elapsed":1912481,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"9c393bd9-c6bc-474c-817e-4130a12bb676"},"source":["# compile the model\r\n","model.compile(optimizer = RMSprop(lr = 0.0001),\r\n","              loss = 'binary_crossentropy',\r\n","              metrics = ['acc'])\r\n","\r\n","# train the model (adjust the number of epochs from 1 to improve performance)\r\n","history = model.fit(\r\n","            train_generator,\r\n","            validation_data = validation_generator,\r\n","            epochs = 5,\r\n","            verbose = 1)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","167/176 [===========================>..] - ETA: 9s - loss: 0.1593 - acc: 0.9345 "],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n","  warnings.warn(str(msg))\n"],"name":"stderr"},{"output_type":"stream","text":["176/176 [==============================] - 196s 1s/step - loss: 0.1588 - acc: 0.9346 - val_loss: 0.0555 - val_acc: 0.9788\n","Epoch 2/10\n","176/176 [==============================] - 191s 1s/step - loss: 0.1271 - acc: 0.9482 - val_loss: 0.0551 - val_acc: 0.9808\n","Epoch 3/10\n","176/176 [==============================] - 191s 1s/step - loss: 0.1109 - acc: 0.9546 - val_loss: 0.0593 - val_acc: 0.9748\n","Epoch 4/10\n","176/176 [==============================] - 192s 1s/step - loss: 0.1035 - acc: 0.9583 - val_loss: 0.0535 - val_acc: 0.9788\n","Epoch 5/10\n","176/176 [==============================] - 191s 1s/step - loss: 0.1037 - acc: 0.9598 - val_loss: 0.0522 - val_acc: 0.9804\n","Epoch 6/10\n","176/176 [==============================] - 190s 1s/step - loss: 0.0937 - acc: 0.9622 - val_loss: 0.0590 - val_acc: 0.9788\n","Epoch 7/10\n","176/176 [==============================] - 190s 1s/step - loss: 0.0844 - acc: 0.9672 - val_loss: 0.0614 - val_acc: 0.9756\n","Epoch 8/10\n","176/176 [==============================] - 190s 1s/step - loss: 0.0760 - acc: 0.9706 - val_loss: 0.0563 - val_acc: 0.9764\n","Epoch 9/10\n","176/176 [==============================] - 190s 1s/step - loss: 0.0822 - acc: 0.9676 - val_loss: 0.0491 - val_acc: 0.9804\n","Epoch 10/10\n","176/176 [==============================] - 190s 1s/step - loss: 0.0809 - acc: 0.9685 - val_loss: 0.0561 - val_acc: 0.9800\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H6Oo6kM-JVQR"},"source":["### Visualize the training and validation accuracy\n","\n","You can see how the training and validation accuracy change with each epoch on an x-y plot."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"id":"erDopoQ5eNL7","executionInfo":{"status":"ok","timestamp":1608980271347,"user_tz":-330,"elapsed":1187,"user":{"displayName":"Ankit Saini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgAXRpKQVXtQ2XI6ir5SXigXYrqz-uUHWrN_CYulg=s64","userId":"07834131228570077607"}},"outputId":"6ae3bba8-62cd-40e8-a830-4993e7cd4cd0"},"source":["%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"xKc_1Qm8JVQR"},"source":["### Predict on a test image\n","\n","You can upload any image and have the model predict whether it's a dog or a cat.\n","- Find an image of a dog or cat\n","- Run the following code cell.  It will ask you to upload an image.\n","- The model will print \"is a dog\" or \"is a cat\" depending on the model's prediction."]},{"cell_type":"code","metadata":{"id":"_0R9fsf4w29e"},"source":["import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(150, 150))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  image_tensor = np.vstack([x])\n","  classes = model.predict(image_tensor)\n","  print(classes)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a dog\")\n","  else:\n","    print(fn + \" is a cat\")"],"execution_count":null,"outputs":[]}]}